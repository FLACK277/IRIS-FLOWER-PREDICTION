{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8784bc1d-072d-4ff4-9b7f-60095119f434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Iris Flower Species Classification\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the path to your iris dataset CSV file (or press Enter to use sample data):  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No file path provided. Using the built-in iris dataset...\n",
      "Dataset loaded with 150 rows and 5 columns\n",
      "\n",
      "First 5 records:\n",
      "   sepal_length  sepal_width  petal_length  petal_width species\n",
      "0           5.1          3.5           1.4          0.2  setosa\n",
      "1           4.9          3.0           1.4          0.2  setosa\n",
      "2           4.7          3.2           1.3          0.2  setosa\n",
      "3           4.6          3.1           1.5          0.2  setosa\n",
      "4           5.0          3.6           1.4          0.2  setosa\n",
      "\n",
      "Data Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   sepal_length  150 non-null    float64\n",
      " 1   sepal_width   150 non-null    float64\n",
      " 2   petal_length  150 non-null    float64\n",
      " 3   petal_width   150 non-null    float64\n",
      " 4   species       150 non-null    object \n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 6.0+ KB\n",
      "None\n",
      "\n",
      "Statistical Summary:\n",
      "       sepal_length  sepal_width  petal_length  petal_width\n",
      "count    150.000000   150.000000    150.000000   150.000000\n",
      "mean       5.843333     3.057333      3.758000     1.199333\n",
      "std        0.828066     0.435866      1.765298     0.762238\n",
      "min        4.300000     2.000000      1.000000     0.100000\n",
      "25%        5.100000     2.800000      1.600000     0.300000\n",
      "50%        5.800000     3.000000      4.350000     1.300000\n",
      "75%        6.400000     3.300000      5.100000     1.800000\n",
      "max        7.900000     4.400000      6.900000     2.500000\n",
      "\n",
      "Class Distribution:\n",
      "species\n",
      "setosa        50\n",
      "versicolor    50\n",
      "virginica     50\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Checking for missing values:\n",
      "sepal_length    0\n",
      "sepal_width     0\n",
      "petal_length    0\n",
      "petal_width     0\n",
      "species         0\n",
      "dtype: int64\n",
      "Creating pairplot of features...\n",
      "Visualizations saved as PNG files.\n",
      "Training set size: 120 samples\n",
      "Testing set size: 30 samples\n",
      "\n",
      "Training Logistic Regression...\n",
      "Logistic Regression Results:\n",
      "  Test Accuracy: 0.9333\n",
      "  Cross-validation Accuracy: 0.9583 ± 0.0264\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        10\n",
      "  versicolor       0.90      0.90      0.90        10\n",
      "   virginica       0.90      0.90      0.90        10\n",
      "\n",
      "    accuracy                           0.93        30\n",
      "   macro avg       0.93      0.93      0.93        30\n",
      "weighted avg       0.93      0.93      0.93        30\n",
      "\n",
      "\n",
      "Training Decision Tree...\n",
      "Decision Tree Results:\n",
      "  Test Accuracy: 0.9333\n",
      "  Cross-validation Accuracy: 0.9417 ± 0.0204\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        10\n",
      "  versicolor       0.90      0.90      0.90        10\n",
      "   virginica       0.90      0.90      0.90        10\n",
      "\n",
      "    accuracy                           0.93        30\n",
      "   macro avg       0.93      0.93      0.93        30\n",
      "weighted avg       0.93      0.93      0.93        30\n",
      "\n",
      "\n",
      "Training Random Forest...\n",
      "Random Forest Results:\n",
      "  Test Accuracy: 0.9000\n",
      "  Cross-validation Accuracy: 0.9500 ± 0.0167\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        10\n",
      "  versicolor       0.82      0.90      0.86        10\n",
      "   virginica       0.89      0.80      0.84        10\n",
      "\n",
      "    accuracy                           0.90        30\n",
      "   macro avg       0.90      0.90      0.90        30\n",
      "weighted avg       0.90      0.90      0.90        30\n",
      "\n",
      "\n",
      "Training SVM...\n",
      "SVM Results:\n",
      "  Test Accuracy: 0.9667\n",
      "  Cross-validation Accuracy: 0.9667 ± 0.0312\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        10\n",
      "  versicolor       1.00      0.90      0.95        10\n",
      "   virginica       0.91      1.00      0.95        10\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.97      0.97        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n",
      "\n",
      "Training K-Nearest Neighbors...\n",
      "K-Nearest Neighbors Results:\n",
      "  Test Accuracy: 0.9333\n",
      "  Cross-validation Accuracy: 0.9667 ± 0.0312\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        10\n",
      "  versicolor       0.83      1.00      0.91        10\n",
      "   virginica       1.00      0.80      0.89        10\n",
      "\n",
      "    accuracy                           0.93        30\n",
      "   macro avg       0.94      0.93      0.93        30\n",
      "weighted avg       0.94      0.93      0.93        30\n",
      "\n",
      "\n",
      "Best model: SVM with accuracy: 0.9667\n",
      "\n",
      "Performing hyperparameter tuning for SVM...\n",
      "Best parameters: {'model__C': 0.1, 'model__gamma': 'scale', 'model__kernel': 'linear'}\n",
      "Best cross-validation accuracy: 0.9750\n",
      "Tuned model test accuracy: 0.9333\n",
      "\n",
      "Tuned Model Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        10\n",
      "  versicolor       0.90      0.90      0.90        10\n",
      "   virginica       0.90      0.90      0.90        10\n",
      "\n",
      "    accuracy                           0.93        30\n",
      "   macro avg       0.93      0.93      0.93        30\n",
      "weighted avg       0.93      0.93      0.93        30\n",
      "\n",
      "\n",
      "SVM doesn't provide direct feature importance.\n",
      "Using permutation importance instead...\n",
      "Permutation Feature Importance:\n",
      "        Feature  Importance\n",
      "3   petal_width    0.295333\n",
      "2  petal_length    0.232667\n",
      "1   sepal_width    0.032000\n",
      "0  sepal_length    0.016000\n",
      "Model saved successfully to iris_classification_model.pkl\n",
      "\n",
      "Testing prediction function with sample data:\n",
      "\n",
      "For measurements: sepal_length=5.1, sepal_width=3.5, petal_length=1.4, petal_width=0.2\n",
      "Predicted species: setosa\n",
      "\n",
      "For measurements: sepal_length=6.0, sepal_width=2.9, petal_length=4.5, petal_width=1.5\n",
      "Predicted species: versicolor\n",
      "\n",
      "For measurements: sepal_length=6.7, sepal_width=3.1, petal_length=5.6, petal_width=2.4\n",
      "Predicted species: virginica\n",
      "\n",
      "Iris classification workflow completed successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#  Data Loading and Exploration\n",
    "def load_and_explore_data(file_path=None):\n",
    "    \"\"\"\n",
    "    Load the iris dataset and perform initial exploration\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if file_path:\n",
    "            # Try to load the provided file\n",
    "            df = pd.read_csv('Downloads/IRIS.csv')\n",
    "            print(f\"Dataset loaded successfully from {file_path}\")\n",
    "        else:\n",
    "            # Use the built-in iris dataset from seaborn\n",
    "            print(\"No file path provided. Using the built-in iris dataset...\")\n",
    "            df = sns.load_dataset('iris')\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        print(\"Creating a sample iris dataset for testing...\")\n",
    "        \n",
    "        # Create a sample iris dataset with the correct column names\n",
    "        data = {\n",
    "            'sepal_length': [5.1, 4.9, 4.7, 7.0, 6.4, 6.9, 6.3, 5.8, 7.1],\n",
    "            'sepal_width': [3.5, 3.0, 3.2, 3.2, 3.2, 3.1, 2.5, 2.7, 3.0],\n",
    "            'petal_length': [1.4, 1.4, 1.3, 4.7, 4.5, 4.9, 6.0, 5.1, 5.9],\n",
    "            'petal_width': [0.2, 0.2, 0.2, 1.4, 1.5, 1.5, 2.5, 1.9, 2.1],\n",
    "            'species': ['setosa', 'setosa', 'setosa', 'versicolor', 'versicolor', 'versicolor', 'virginica', 'virginica', 'virginica']\n",
    "        }\n",
    "        df = pd.DataFrame(data)\n",
    "    \n",
    "    print(f\"Dataset loaded with {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "    \n",
    "    print(\"\\nFirst 5 records:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    print(\"\\nData Information:\")\n",
    "    print(df.info())\n",
    "    \n",
    "    print(\"\\nStatistical Summary:\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    print(\"\\nClass Distribution:\")\n",
    "    print(df['species'].value_counts())\n",
    "    \n",
    "    print(\"\\nChecking for missing values:\")\n",
    "    print(df.isnull().sum())\n",
    "    \n",
    "    return df\n",
    "\n",
    "#  Data Visualization\n",
    "def visualize_data(df):\n",
    "    \"\"\"\n",
    "    Create visualizations to understand the dataset better\n",
    "    \"\"\"\n",
    "    # Set up the matplotlib figure size\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # 2.1 Create a pairplot to visualize relationships between features by species\n",
    "    print(\"Creating pairplot of features...\")\n",
    "    pairplot = sns.pairplot(df, hue='species', height=2.5)\n",
    "    pairplot.fig.suptitle('Pairwise Relationships between Features', y=1.02)\n",
    "    plt.savefig('iris_pairplot.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2.2 Create boxplots for each feature by species\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.suptitle('Feature Distributions by Species', y=0.95, fontsize=16)\n",
    "    \n",
    "    features = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "    \n",
    "    for i, feature in enumerate(features):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        sns.boxplot(x='species', y=feature, data=df)\n",
    "        plt.title(f'{feature.replace(\"_\", \" \").title()} by Species')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('iris_boxplots.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2.3 Create a correlation heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    correlation = numeric_df.corr()\n",
    "    \n",
    "    sns.heatmap(correlation, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "    plt.title('Correlation Heatmap of Features', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('iris_correlation.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Visualizations saved as PNG files.\")\n",
    "\n",
    "#  Data Preprocessing\n",
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess the data for modeling\n",
    "    \"\"\"\n",
    "    # 3.1 Create features and target variables\n",
    "    X = df.drop('species', axis=1)\n",
    "    y = df['species']\n",
    "    \n",
    "    # 3.2 Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "    print(f\"Testing set size: {X_test.shape[0]} samples\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, X, y\n",
    "\n",
    "#  Model Building and Evaluation\n",
    "def build_and_evaluate_models(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Build and evaluate multiple classification models\n",
    "    \"\"\"\n",
    "    # 4.1 Define the models to evaluate\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(max_iter=200, random_state=42),\n",
    "        'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "        'Random Forest': RandomForestClassifier(random_state=42),\n",
    "        'SVM': SVC(random_state=42),\n",
    "        'K-Nearest Neighbors': KNeighborsClassifier()\n",
    "    }\n",
    "    \n",
    "    #  Created a dictionary to store the results\n",
    "    results = {}\n",
    "    best_accuracy = 0\n",
    "    best_model_name = None\n",
    "    best_model = None\n",
    "    \n",
    "    # 4.3 Train and evaluate each model\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        \n",
    "        # Create a pipeline with scaling\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', model)\n",
    "        ])\n",
    "        \n",
    "        # Train the model\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        # Run cross-validation\n",
    "        cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"{name} Results:\")\n",
    "        print(f\"  Test Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"  Cross-validation Accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        \n",
    "        # Create confusion matrix\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=sorted(df['species'].unique()),\n",
    "                    yticklabels=sorted(df['species'].unique()))\n",
    "        plt.title(f'Confusion Matrix - {name}')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'confusion_matrix_{name.replace(\" \", \"_\").lower()}.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Store the results\n",
    "        results[name] = {\n",
    "            'model': pipeline,\n",
    "            'accuracy': accuracy,\n",
    "            'cv_scores': cv_scores,\n",
    "            'predictions': y_pred\n",
    "        }\n",
    "        \n",
    "        # Keep track of the best model\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model_name = name\n",
    "            best_model = pipeline\n",
    "    \n",
    "    print(f\"\\nBest model: {best_model_name} with accuracy: {best_accuracy:.4f}\")\n",
    "    return results, best_model_name, best_model\n",
    "\n",
    "#  Hyperparameter Tuning\n",
    "def tune_best_model(best_model_name, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning on the best model\n",
    "    \"\"\"\n",
    "    # Define parameter grids based on the best model\n",
    "    param_grids = {\n",
    "        'Logistic Regression': {\n",
    "            'model__C': [0.01, 0.1, 1, 10, 100],\n",
    "            'model__solver': ['liblinear', 'lbfgs']\n",
    "        },\n",
    "        'Decision Tree': {\n",
    "            'model__max_depth': [None, 5, 10, 15],\n",
    "            'model__min_samples_split': [2, 5, 10],\n",
    "            'model__min_samples_leaf': [1, 2, 4]\n",
    "        },\n",
    "        'Random Forest': {\n",
    "            'model__n_estimators': [50, 100, 200],\n",
    "            'model__max_depth': [None, 10, 20],\n",
    "            'model__min_samples_split': [2, 5]\n",
    "        },\n",
    "        'SVM': {\n",
    "            'model__C': [0.1, 1, 10],\n",
    "            'model__kernel': ['linear', 'rbf', 'poly'],\n",
    "            'model__gamma': ['scale', 'auto', 0.1, 1]\n",
    "        },\n",
    "        'K-Nearest Neighbors': {\n",
    "            'model__n_neighbors': [3, 5, 7, 9],\n",
    "            'model__weights': ['uniform', 'distance'],\n",
    "            'model__metric': ['euclidean', 'manhattan']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Check if the best model has a parameter grid\n",
    "    if best_model_name in param_grids:\n",
    "        print(f\"\\nPerforming hyperparameter tuning for {best_model_name}...\")\n",
    "        \n",
    "        # Get the base pipeline structure from the best model\n",
    "        if best_model_name == 'Logistic Regression':\n",
    "            model = LogisticRegression(random_state=42)\n",
    "        elif best_model_name == 'Decision Tree':\n",
    "            model = DecisionTreeClassifier(random_state=42)\n",
    "        elif best_model_name == 'Random Forest':\n",
    "            model = RandomForestClassifier(random_state=42)\n",
    "        elif best_model_name == 'SVM':\n",
    "            model = SVC(random_state=42)\n",
    "        elif best_model_name == 'K-Nearest Neighbors':\n",
    "            model = KNeighborsClassifier()\n",
    "        \n",
    "        # Create a new pipeline for grid search\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', model)\n",
    "        ])\n",
    "        \n",
    "        # Set up grid search\n",
    "        grid_search = GridSearchCV(\n",
    "            pipeline,\n",
    "            param_grids[best_model_name],\n",
    "            cv=5,\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Fit grid search\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Get best parameters and score\n",
    "        print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "        print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
    "        \n",
    "        # Evaluate the tuned model\n",
    "        tuned_model = grid_search.best_estimator_\n",
    "        tuned_y_pred = tuned_model.predict(X_test)\n",
    "        tuned_accuracy = accuracy_score(y_test, tuned_y_pred)\n",
    "        \n",
    "        print(f\"Tuned model test accuracy: {tuned_accuracy:.4f}\")\n",
    "        print(\"\\nTuned Model Classification Report:\")\n",
    "        print(classification_report(y_test, tuned_y_pred))\n",
    "        \n",
    "        # Create confusion matrix for tuned model\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        cm = confusion_matrix(y_test, tuned_y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=sorted(df['species'].unique()),\n",
    "                    yticklabels=sorted(df['species'].unique()))\n",
    "        plt.title(f'Confusion Matrix - Tuned {best_model_name}')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'confusion_matrix_tuned_{best_model_name.replace(\" \", \"_\").lower()}.png')\n",
    "        plt.close()\n",
    "        \n",
    "        return tuned_model, tuned_accuracy\n",
    "    else:\n",
    "        print(f\"No parameter grid defined for {best_model_name}. Skipping hyperparameter tuning.\")\n",
    "        return None, None\n",
    "\n",
    "#  Feature Importance Analysis\n",
    "def analyze_feature_importance(df, best_model, best_model_name):\n",
    "    \"\"\"\n",
    "    Analyze and visualize feature importance\n",
    "    \"\"\"\n",
    "    feature_names = df.drop('species', axis=1).columns\n",
    "    \n",
    "    # Different models have different ways to access feature importance\n",
    "    if best_model_name == 'Logistic Regression':\n",
    "        # For multiclass, coefficients are n_classes * n_features\n",
    "        # We take the absolute mean across classes\n",
    "        try:\n",
    "            coefficients = best_model.named_steps['model'].coef_\n",
    "            importance = np.abs(coefficients).mean(axis=0)\n",
    "            \n",
    "            # Create a dataframe for better visualization\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'Feature': feature_names,\n",
    "                'Importance': importance\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "            \n",
    "            print(\"\\nFeature Importance (Logistic Regression):\")\n",
    "            print(feature_importance)\n",
    "        except:\n",
    "            print(\"Could not extract feature importance from Logistic Regression model.\")\n",
    "            return None\n",
    "            \n",
    "    elif best_model_name in ['Decision Tree', 'Random Forest']:\n",
    "        try:\n",
    "            importance = best_model.named_steps['model'].feature_importances_\n",
    "            \n",
    "            # Create a dataframe for better visualization\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'Feature': feature_names,\n",
    "                'Importance': importance\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "            \n",
    "            print(f\"\\nFeature Importance ({best_model_name}):\")\n",
    "            print(feature_importance)\n",
    "        except:\n",
    "            print(f\"Could not extract feature importance from {best_model_name} model.\")\n",
    "            return None\n",
    "    else:\n",
    "        # For models without direct feature importance\n",
    "        print(f\"\\n{best_model_name} doesn't provide direct feature importance.\")\n",
    "        print(\"Using permutation importance instead...\")\n",
    "        \n",
    "        try:\n",
    "            from sklearn.inspection import permutation_importance\n",
    "            \n",
    "            X = df.drop('species', axis=1)\n",
    "            y = df['species']\n",
    "            \n",
    "            # Calculate permutation importance\n",
    "            result = permutation_importance(best_model, X, y, n_repeats=10, random_state=42)\n",
    "            importance = result.importances_mean\n",
    "            \n",
    "            # Create a dataframe for better visualization\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'Feature': feature_names,\n",
    "                'Importance': importance\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "            \n",
    "            print(\"Permutation Feature Importance:\")\n",
    "            print(feature_importance)\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating permutation importance: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # Plot feature importance\n",
    "    try:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
    "        plt.title(f'Feature Importance - {best_model_name}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('feature_importance.png')\n",
    "        plt.close()\n",
    "        \n",
    "        return feature_importance\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting feature importance: {e}\")\n",
    "        return None\n",
    "\n",
    "# Save the Model\n",
    "def save_model(model, filename='iris_classification_model.pkl'):\n",
    "    \"\"\"\n",
    "    Save the trained model to a file\n",
    "    \"\"\"\n",
    "    import pickle\n",
    "    \n",
    "    try:\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        print(f\"Model saved successfully to {filename}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# 8. Create a prediction function\n",
    "def predict_species(model, sepal_length, sepal_width, petal_length, petal_width):\n",
    "    \"\"\"\n",
    "    Make a prediction using the trained model\n",
    "    \"\"\"\n",
    "    # Create a DataFrame with the input features\n",
    "    input_data = pd.DataFrame({\n",
    "        'sepal_length': [sepal_length],\n",
    "        'sepal_width': [sepal_width],\n",
    "        'petal_length': [petal_length],\n",
    "        'petal_width': [petal_width]\n",
    "    })\n",
    "    \n",
    "    # Make prediction\n",
    "    species = model.predict(input_data)[0]\n",
    "    \n",
    "    # Initialize prob_dict to None by default\n",
    "    prob_dict = None\n",
    "    \n",
    "    # If the model provides probability estimates, get them\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        try:\n",
    "            probabilities = model.predict_proba(input_data)[0]\n",
    "            # Get class labels to match with probabilities\n",
    "            class_labels = model.classes_\n",
    "            prob_dict = {class_labels[i]: probabilities[i] for i in range(len(class_labels))}\n",
    "        except:\n",
    "            # prob_dict remains None if there's an exception\n",
    "            pass\n",
    "    \n",
    "    return species, prob_dict\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Run the full iris classification pipeline\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"Iris Flower Species Classification\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Ask for file path\n",
    "    file_path = input(\"Enter the path to your iris dataset CSV file (or press Enter to use sample data): \")\n",
    "    if not file_path.strip():\n",
    "        file_path = None\n",
    "    \n",
    "    # 1. Load and explore data\n",
    "    global df  # Make df accessible to all functions\n",
    "    df = load_and_explore_data(file_path)\n",
    "    \n",
    "    # 2. Visualize data\n",
    "    visualize_data(df)\n",
    "    \n",
    "    # 3. Preprocess data\n",
    "    X_train, X_test, y_train, y_test, X, y = preprocess_data(df)\n",
    "    \n",
    "    # 4. Build and evaluate models\n",
    "    results, best_model_name, best_model = build_and_evaluate_models(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # 5. Tune the best model\n",
    "    tuned_model, tuned_accuracy = tune_best_model(best_model_name, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # Use the tuned model if it's better, otherwise use the original best model\n",
    "    final_model = tuned_model if tuned_model and tuned_accuracy > results[best_model_name]['accuracy'] else best_model\n",
    "    final_model_name = f\"Tuned {best_model_name}\" if tuned_model and tuned_accuracy > results[best_model_name]['accuracy'] else best_model_name\n",
    "    \n",
    "    # 6. Analyze feature importance\n",
    "    feature_importance = analyze_feature_importance(df, final_model, best_model_name)\n",
    "    \n",
    "    # 7. Save the model\n",
    "    save_model(final_model)\n",
    "    \n",
    "    # 8. Test the prediction function\n",
    "    print(\"\\nTesting prediction function with sample data:\")\n",
    "    # Sample for Setosa\n",
    "    setosa_sample = [5.1, 3.5, 1.4, 0.2]\n",
    "    # Sample for Versicolor\n",
    "    versicolor_sample = [6.0, 2.9, 4.5, 1.5]\n",
    "    # Sample for Virginica\n",
    "    virginica_sample = [6.7, 3.1, 5.6, 2.4]\n",
    "    \n",
    "    for sample in [setosa_sample, versicolor_sample, virginica_sample]:\n",
    "        species, probs = predict_species(final_model, *sample)\n",
    "        print(f\"\\nFor measurements: sepal_length={sample[0]}, sepal_width={sample[1]}, petal_length={sample[2]}, petal_width={sample[3]}\")\n",
    "        print(f\"Predicted species: {species}\")\n",
    "        if probs:\n",
    "            print(\"Prediction probabilities:\")\n",
    "            for species_name, prob in probs.items():\n",
    "                print(f\"  - {species_name}: {prob:.4f}\")\n",
    "    \n",
    "    print(\"\\nIris classification workflow completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2fc0c3-f79b-4ff9-b179-12d8bb4294fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
